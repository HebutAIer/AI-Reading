# 2. Linear Algebra
* If you have previous experience with these concepts but need a detailed reference sheet to review key formulas, [The Matrix Cookbook](http://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf) is recommend.
## 2.1 Scalars, Vectors, Matrices and Tensors
1. Scalars: a single number
2. Vectors: an array of numbers, It's writen as a column enclosed in square brackets
3.  Matrices: a 2-D array of numbers, $A_{i,:}$ is the ith row and $A_{:,i}$ is the ith column
4. Tensors: an array with more than two axes
## 2.2 Matrix operation
1. Transpose: The transpose of a matrix is the mirror image of the matrix across a diagonal line
2. Broadcasting: 
3. Multiplying Matrices and Vectors
4. Element wise product
5. Dot product
## 2.3 Identity and Inverse Matrices
1. Identity matrix:
2. Matrix inverse:
    * The same inverse matrix can then be used to solve the equation many times for different values of b in Ax=b
    * $A^{-1}$is primarily useful as a theoretical tool and should not actually be used in practice for most software applications. 
        * Because $A^{-1}$ can be represented with only limited precision on a digital computer
        * Algorithms that make use of the value of b can usually obtain more accurate estimates of x
## 2.4 Linear Dependence and Span
1. It is not possible to have more than one but less than infinitely many solutions for a particular b. if x and y are solutions, then $z=ax+(1-a)y$ is also a solution for any real a
2. To analyze how solutions the equation has:
    * Think of the columns of A as specifying different directions we can travel in from the origin
    * Determine how many ways there are of reaching b
    * Each element of x specifies how far we should travel in each of these directions with $x_i$ specifying how far to move in the direction of column i
3. Linear combination: $Ax=\sum_{i}x_iA_{:,i}$
4. Span: the span of a set of vectors is the set of all points obtainable by linear combination of the original vectors
    * Determining whether $Ax=b$ has a solution thus amounts to testing whether b is in the span of the columns of A, 
    * This particular span is known as the column space, or the range of A. 
    * In order for the system $Ax=b$ has a solution thus amounts to testing whether b is in the span of the columns of A
5. Linearly independent: if no vector in the set is a linear combination of the other vectors. If we add a vector to a set that is a linear combination of the other vectors in the set, the new vector does not add any points to the set's span
6. THe matrix contain at least one set of m linearly independent columns $\Leftrightarrow$ $Ax=b$ has a solution for every value of b
## 2.3 Norms
1. Norm is a function: 
    1. measure the size of a vector.
    2. measure the distance from the origin to the point x.
    3. mapping vectors to non-negative values.
    4. satisfies the following properties:
        1. $f(x)=0 \Rightarrow x=0$
        2. $f(x+y)\le f(x)+f(y)$
        3. $\forall \alpha \in R, f\left(\alpha x\right)=|\alpha|f(x)$
2. $L^p=||x||_p=\left(\sum_{i}|x_i|^p\right)^{1/p}$
    1. $||x||_2^2=x^Tx$
        1. increases very slowly near the origin.
    2. $||x||_1=\sum_i|x_i|$
        1. grows at the same rate in all locations.
        2. used in machine learning when the difference between zero and nozero elements is very important.
        3. every time an element of x moves away from 0 by $\epsilon$, the $L^1$ norm increases by $\epsilon$.
    3. $||x||_0=the\  number\  of\  nonzero\  elements$
        1. The number of nonzero entries in a vector is not a norm, because scaling the vector by $\alpha$ does not change the number of nonzero entries.
        2. $L^1$ norm is often used as a substitute for the number of nonzero entries.
    4. $||x||_\infty=\max|x_i|$
    5. $||A||_F=\sqrt{\sum_{ij}A^2_{ij}}$
    6. $x^Ty=||x||_2||y||_2cos\theta$
## 2.4 Special Kinds of Matrices and Vectors
1. Diagonal: a matrix D is diagonal if and only if $D_{i,j}=0$ for $i\ne j$.
    1. In many cases, we may derive some general machine learning algorithm in terms of arbitrary matrices but obtain a less expensive algorithm by restricting some matrices to be diagonal.
2. Symmetric: $A=A^T$
    1. Often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments.
3. Unit vector: $||x||_2=1$
4. Orthogonal: $x^Ty=0$, if both vectors have nonzero norm.
    1. In $R^n$ at most n vectors may be mutually orthogonal with nonzero norm.
5. Orthonormal: if the vectors not only are orthogonal but also have unit norm
6. Orthononal matrix: $A^TA=AA^T=I$
    1. Their rows are not merely orthogonal but fully orthonormal.
## 2.5 Eigendecomposition
1. Constructing matrices with specific eigenvalues and eigenvectors enables us to stretch space in desired directions.
2. Every real eymmetric matrix can be decomposed into an expression using only real-valued eigenvectors, but may not be unique.
3. If eigendecomposition is unique only if all the eigenvalues are unique.
4. In $A=Q\Lambda Q^T$, $A$ can be think as scaling space by $\lambda_i$ in direction $v^{i}$.
5. The matrix is singular if and only if any of the eigenvalues are zero.
6. The eigendecomposition of a real symmetric matrix can also be used to optimize quadratic expressions of the form $f(x)=x^TAx$ subject to $||x||_2=1$
    1. The maximum value of $f$ within the constraint region is the maximum eigenvalue.
    2. The minimum value of $f$ within the constraint region is the minimum eigenvalue.
## 2.8 Singular Value Decomposition
1. Every real matrix has a singular value decomposition.
2. The most useful feature of the SVD is that we can use it to partially generalize matrix inversion to nonsquare matrices.
## 2.9 The Trace Operator
1. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator.
2. $Tr(A)=Tr(A^T)
3. $\alpha=Tr(\alpha)$
4. $Tr(\prod_{i=1}^{n}F^i)=Tr(F^n\prod_{i=1}^{n-1}F^i)$, this invariance to cyclic permutation holds even if the resulting product has a different shape.
## 2.10 Determinant
1. The determinant is equal to the product of all the eigenvalues of the matrix.
2. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space.(from eigendecomposition perspect)
    1. if the determinant is 0, then space is contracted completely along at least one dimension, casing it to lose all its volume.
    2. if the determinant is 1, then the transformation preserves volume.
## 2.11 Example: PCA
